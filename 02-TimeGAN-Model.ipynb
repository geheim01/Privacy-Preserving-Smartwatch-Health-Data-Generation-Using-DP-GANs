{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02: TimeGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of the TimeGAN is based on the [TimeGAN-tensorflow2-Repository](https://github.com/mcps5601/TimeGAN-tensorflow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthesizers.timegan.timegan import train_timegan\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = np.load('data/wesad/wesad_windows.npy')\n",
    "labels = np.load('data/wesad/wesad_labels.npy')\n",
    "\n",
    "stress_features = windows[labels == 1]\n",
    "non_stress_features = windows[labels == 0]\n",
    "\n",
    "stress_features = np.delete(stress_features, 6, axis=2)\n",
    "non_stress_features = np.delete(non_stress_features, 6, axis=2)\n",
    "\n",
    "stress_features.shape, non_stress_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for the main function\n",
    "EXP_NAME = 'timegan_test'\n",
    "DATA_NAME = 'wesad'\n",
    "USE_GAIN = False\n",
    "MAX_SEQ_LEN = 60\n",
    "IMP_METHOD = 'median'\n",
    "TRAIN_RATE = 1\n",
    "FEATURE_PREDICTION_NO = 5\n",
    "SEED = 0\n",
    "HIDER_MODEL = 'timegan'\n",
    "NOISE_SIZE = 0.1\n",
    "SEEKER_MODEL = 'binary_predictor'\n",
    "TRAIN_ON_STRESS_DATA = False\n",
    "\n",
    "# Hider params\n",
    "GEN_TYPE = 'gan'\n",
    "MODULE_NAME = 'gru'\n",
    "EPSILON = 1e-8\n",
    "OPTIMIZER = 'adam'\n",
    "USE_DPSGD = False\n",
    "BATCH_SIZE = 128\n",
    "Z_DIM = -1\n",
    "HIDDEN_DIM = 10\n",
    "NUM_LAYERS = 3\n",
    "EMBEDDING_ITERATIONS = 2000\n",
    "SUPERVISED_ITERATIONS = 1000\n",
    "JOINT_ITERATIONS = 6000\n",
    "ETA = 0.1\n",
    "NORMALIZATION = False\n",
    "\n",
    "# DP params\n",
    "DP_TRAINING = False\n",
    "L2_NORM_CLIP = 1.0\n",
    "NOISE_MULTIPLIER = 0.1\n",
    "DP_LR = 0.15\n",
    "\n",
    "# Additional variables\n",
    "FEATURE_DIM = stress_features.shape[-1]\n",
    "\n",
    "config = {\n",
    "    'exp_name': EXP_NAME,\n",
    "    'data_name': DATA_NAME,\n",
    "    'use_gain': USE_GAIN,\n",
    "    'max_seq_len': MAX_SEQ_LEN,\n",
    "    'imp_method': IMP_METHOD,\n",
    "    'train_rate': TRAIN_RATE,\n",
    "    'feature_prediction_no': FEATURE_PREDICTION_NO,\n",
    "    'seed': SEED,\n",
    "    'hider_model': HIDER_MODEL,\n",
    "    'noise_size': NOISE_SIZE,\n",
    "    'seeker_model': SEEKER_MODEL,\n",
    "    'gen_type': GEN_TYPE,\n",
    "    'module_name': MODULE_NAME,\n",
    "    'epsilon': EPSILON,\n",
    "    'optimizer': OPTIMIZER,\n",
    "    'use_dpsgd': USE_DPSGD,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'z_dim': Z_DIM,\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'embedding_iterations': EMBEDDING_ITERATIONS,\n",
    "    'supervised_iterations': SUPERVISED_ITERATIONS,\n",
    "    'joint_iterations': JOINT_ITERATIONS,\n",
    "    'eta': ETA,\n",
    "    'l2_norm_clip': L2_NORM_CLIP,\n",
    "    'noise_multiplier': NOISE_MULTIPLIER,\n",
    "    'dp_lr': DP_LR,\n",
    "    'feature_dim': FEATURE_DIM,\n",
    "    'dp_training': DP_TRAINING,\n",
    "    'normalization': NORMALIZATION,\n",
    "    'train_on_stress_data': TRAIN_ON_STRESS_DATA,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def data_division (data, seed, divide_rates):\n",
    "  \"\"\"Divide the dataset into sub datasets.\n",
    "  \n",
    "  Args:\n",
    "    - data: original data (list format)\n",
    "    - seed: random seed\n",
    "    - divide_rates: ratio for each division\n",
    "    \n",
    "  Returns:\n",
    "    - divided_data: divided data (list format)\n",
    "    - divided_index: divided data index (list format)\n",
    "  \"\"\"\n",
    "  # sum of the division rates should be 1\n",
    "  assert sum(divide_rates) == 1\n",
    "  \n",
    "  # Output initialization\n",
    "  divided_data = list()\n",
    "  divided_index = list()\n",
    "  \n",
    "  # Set index\n",
    "  no = len(data)\n",
    "  random.seed(seed)\n",
    "  index = np.random.permutation(no)\n",
    "\n",
    "  # Set divided index & data\n",
    "  for i in range(len(divide_rates)):\n",
    "    temp_idx = index[int(no*sum(divide_rates[:i])):int(no*sum(divide_rates[:(i+1)]))]\n",
    "    divided_index.append(temp_idx)\n",
    "    \n",
    "    temp_data = [data[j] for j in temp_idx]\n",
    "    divided_data.append(temp_data)\n",
    "  \n",
    "  return divided_data, divided_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish data loading: wesad\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into training and testing\n",
    "\n",
    "divided_data, _ = data_division(stress_features if TRAIN_ON_STRESS_DATA else non_stress_features, \n",
    "                                seed=SEED, \n",
    "                                divide_rates=[TRAIN_RATE, 1-TRAIN_RATE])\n",
    "\n",
    "train_data = np.asarray(divided_data[0])\n",
    "test_data = np.asarray(divided_data[1])\n",
    "\n",
    "train_data.shape, non_stress_features.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnw20hewo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/nils/thesis/Data_Generation/wandb/run-20230726_031241-h72ut5xj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nw20hewo/timegan/runs/h72ut5xj' target=\"_blank\">rosy-snowball-44</a></strong> to <a href='https://wandb.ai/nw20hewo/timegan' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nw20hewo/timegan' target=\"_blank\">https://wandb.ai/nw20hewo/timegan</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nw20hewo/timegan/runs/h72ut5xj' target=\"_blank\">https://wandb.ai/nw20hewo/timegan/runs/h72ut5xj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING WITH FOLLOWING CONFIG:\n",
      "{'exp_name': 'timegan_test', 'data_name': 'wesad', 'use_gain': False, 'max_seq_len': 60, 'imp_method': 'median', 'train_rate': 1, 'feature_prediction_no': 5, 'seed': 0, 'hider_model': 'timegan', 'noise_size': 0.1, 'seeker_model': 'binary_predictor', 'gen_type': 'gan', 'module_name': 'gru', 'epsilon': 1e-08, 'optimizer': 'adam', 'use_dpsgd': False, 'batch_size': 128, 'z_dim': -1, 'hidden_dim': 10, 'num_layers': 3, 'embedding_iterations': 2000, 'supervised_iterations': 1000, 'joint_iterations': 6000, 'eta': 0.1, 'l2_norm_clip': 1.0, 'noise_multiplier': 0.1, 'dp_lr': 0.15, 'feature_dim': 6, 'dp_training': False, 'normalization': False, 'train_on_stress_data': False}\n",
      "NO DATA normalized\n",
      "Start Embedding Network Training\n",
      "step: 0/2000, e_loss: 0.5501\n",
      "step: 100/2000, e_loss: 0.2081\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, max_val, min_val, train_log_dir \u001b[39m=\u001b[39m train_timegan(train_data, \u001b[39m0\u001b[39;49m, config)\n",
      "File \u001b[0;32m~/thesis/Data_Generation/synthesizers/timegan/timegan.py:362\u001b[0m, in \u001b[0;36mtrain_timegan\u001b[0;34m(ori_data, dynamic_time, args)\u001b[0m\n\u001b[1;32m    358\u001b[0m X_mb, T_mb \u001b[39m=\u001b[39m batch_generator(\n\u001b[1;32m    359\u001b[0m     ori_data, ori_time, args[\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m], use_tf_data\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    360\u001b[0m )\n\u001b[1;32m    361\u001b[0m X_mb \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(X_mb, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m--> 362\u001b[0m step_e_loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mrecovery_forward(X_mb, optimizer)\n\u001b[1;32m    363\u001b[0m \u001b[39mif\u001b[39;00m itt \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    364\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    365\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstep: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(itt)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(np\u001b[39m.\u001b[39mround(np\u001b[39m.\u001b[39msqrt(step_e_loss), \u001b[39m4\u001b[39m))\n\u001b[1;32m    371\u001b[0m     )\n",
      "File \u001b[0;32m~/thesis/Data_Generation/synthesizers/timegan/timegan.py:53\u001b[0m, in \u001b[0;36mTimeGAN.recovery_forward\u001b[0;34m(self, X, optimizer)\u001b[0m\n\u001b[1;32m     50\u001b[0m     E_loss0 \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m \u001b[39m*\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39msqrt(E_loss_T0)\n\u001b[1;32m     52\u001b[0m var_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedder\u001b[39m.\u001b[39mtrainable_weights \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecovery\u001b[39m.\u001b[39mtrainable_weights\n\u001b[0;32m---> 53\u001b[0m grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(E_loss0, var_list)\n\u001b[1;32m     54\u001b[0m optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(grads, var_list))\n\u001b[1;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m E_loss_T0\n",
      "File \u001b[0;32m~/miniconda3/envs/final_requirements/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py:1063\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1057\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[1;32m   1058\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1059\u001b[0m           output_gradients))\n\u001b[1;32m   1060\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1061\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1063\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[1;32m   1064\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[1;32m   1065\u001b[0m     flat_targets,\n\u001b[1;32m   1066\u001b[0m     flat_sources,\n\u001b[1;32m   1067\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[1;32m   1068\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[1;32m   1069\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[1;32m   1071\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[1;32m   1072\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/miniconda3/envs/final_requirements/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[0;32m~/miniconda3/envs/final_requirements/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py:146\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    144\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 146\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/miniconda3/envs/final_requirements/lib/python3.8/site-packages/tensorflow/python/ops/math_grad.py:780\u001b[0m, in \u001b[0;36m_TanhGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcontrol_dependencies([grad]):\n\u001b[1;32m    779\u001b[0m   y \u001b[39m=\u001b[39m math_ops\u001b[39m.\u001b[39mconj(y)\n\u001b[0;32m--> 780\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49mtanh_grad(y, grad)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, max_val, min_val, train_log_dir = train_timegan(train_data, 0, config)\n",
    "os.system(\"say 'TimeGAN IST FERTIG'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_generator(batch_size, z_dim, T_mb, max_seq_len):\n",
    "    \"\"\"Random vector generation.\n",
    "    Args:\n",
    "        batch_size: size of the random vector\n",
    "        z_dim: dimension of random vector\n",
    "        T_mb: time information for the random vector\n",
    "        max_seq_len: maximum sequence length\n",
    "    Return:\n",
    "        Z_mb: generated random vector\n",
    "    \"\"\"\n",
    "    Z_mb = list()\n",
    "    for i in range(batch_size):\n",
    "        temp = np.zeros([max_seq_len, z_dim])\n",
    "        temp_Z = np.random.uniform(0., 1, [T_mb[i], z_dim])\n",
    "        temp[:T_mb[i],:] = temp_Z\n",
    "        Z_mb.append(temp_Z)\n",
    "\n",
    "    return Z_mb\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save non-stress dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# save synthetic data\n",
    "\n",
    "max_seq_len = MAX_SEQ_LEN\n",
    "num_syn_samples = 540\n",
    "ori_time = [60] * num_syn_samples\n",
    "Z_mb = random_generator(num_syn_samples, config['z_dim'], ori_time, max_seq_len)\n",
    "Z_mb = tf.convert_to_tensor(Z_mb, dtype=tf.float32)\n",
    "generated_data = model.generate(Z_mb, num_syn_samples, ori_time, max_val, min_val)\n",
    "zeros = np.zeros([num_syn_samples, 60, 1])\n",
    "gen_data = np.append(np.array(generated_data), zeros, axis=2)\n",
    "\n",
    "with open (f\"/Users/nils/thesis/Data_Generation/data/syn/timegan/no_dp/non_stress/syn_dataset_540.npy\", \"wb\") as f:\n",
    "   np.save(f, gen_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save stress dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = MAX_SEQ_LEN\n",
    "num_syn_samples = 540\n",
    "ori_time = [60] * num_syn_samples\n",
    "Z_mb = random_generator(num_syn_samples, config['z_dim'], ori_time, max_seq_len)\n",
    "Z_mb = tf.convert_to_tensor(Z_mb, dtype=tf.float32)\n",
    "generated_data = model.generate(Z_mb, num_syn_samples, ori_time, max_val, min_val)\n",
    "ones = np.ones([num_syn_samples, 60, 1])\n",
    "gen_data = np.append(np.array(generated_data), ones, axis=2)\n",
    "\n",
    "with open (f\"data/syn/timegan/no_dp/stress/syn_dataset_540.npy\", \"wb\") as f:\n",
    "   np.save(f, gen_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save non-stress subject data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save synthetic data\n",
    "num_syn_samples = 540 // 15\n",
    "ori_time = [60] * num_syn_samples\n",
    "Z_mb = random_generator(num_syn_samples, config['z_dim'], ori_time, max_seq_len)\n",
    "Z_mb = tf.convert_to_tensor(Z_mb, dtype=tf.float32)\n",
    "generated_data = model.generate(Z_mb, num_syn_samples, ori_time, max_val, min_val)\n",
    "zeros = np.zeros([num_syn_samples, 60, 1])\n",
    "gen_data = np.append(np.array(generated_data), zeros, axis=2)\n",
    "\n",
    "with open (f\"data/syn/timegan/no_dp/non_stress/syn_subject_36.npy\", \"wb\") as f:\n",
    "    np.save(f, gen_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save stress subject data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save synthetic data\n",
    "num_syn_samples = 540 // 15\n",
    "ori_time = [60] * num_syn_samples\n",
    "Z_mb = random_generator(num_syn_samples, config['z_dim'], ori_time, max_seq_len)\n",
    "Z_mb = tf.convert_to_tensor(Z_mb, dtype=tf.float32)\n",
    "generated_data = model.generate(Z_mb, num_syn_samples, ori_time, max_val, min_val)\n",
    "ones = np.ones([num_syn_samples, 60, 1])\n",
    "gen_data = np.append(np.array(generated_data), ones, axis=2)\n",
    "\n",
    "with open (f\"data/syn/timegan/no_dp/stress/syn_subject_36.npy\", \"wb\") as f:\n",
    "    np.save(f, gen_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
